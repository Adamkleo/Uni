{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECCIÓN 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARTE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero comprobaremos las predicciones del modelo para valores altos. Nuestro modelo utilizado para esta sección será el escogido en la parte anterior y que se ha guardado en modelo.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold de energía 1089.375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  \n",
    "# Cargamos el modelo final\n",
    "final_model = joblib.load(\"modelo_final.pkl\")\n",
    "\n",
    "# Cargamos el dataset\n",
    "wind_ava = pd.read_csv('wind_ava.csv.gz', compression=\"gzip\")\n",
    "all_columns = wind_ava.columns.tolist()\n",
    "sotavento_columns = ['energy'] + [col for col in all_columns if '.13' in col]\n",
    "\n",
    "wind_ava_sotavento = wind_ava[sotavento_columns]\n",
    "\n",
    "y_full = wind_ava_sotavento['energy']\n",
    "X_full = wind_ava_sotavento.drop('energy', axis=1)\n",
    "\n",
    "y_pred = final_model.predict(X_full)\n",
    "\n",
    "\n",
    "#Consideramos el tercer cuantil el threshold para valores altos de energía\n",
    "\n",
    "energy_threshold= wind_ava_sotavento[\"energy\"].quantile(0.75)\n",
    "print(\"Threshold de energía\",energy_threshold)\n",
    "\n",
    "# Ahora, creamos dos subsets. \n",
    "# Uno de valores considerados bajos y otro con los valores altos de energía\n",
    "\n",
    "# Filas con valor de energía mayor que el threshold\n",
    "high_energy_rows=y_full >= energy_threshold\n",
    "\n",
    "# Filas con valor de energía menor que el threshold\n",
    "low_energy_rows=y_full<energy_threshold\n",
    "\n",
    "# Se crean subsets con estas filas\n",
    "high_energy_subset= X_full[high_energy_rows]\n",
    "low_energy_subset= X_full[low_energy_rows]\n",
    "\n",
    "# Predicciones\n",
    "high_energy_prediction=final_model.predict(high_energy_subset)\n",
    "low_energy_prediction=final_model.predict(low_energy_subset)\n",
    "\n",
    "# Cálculo de errores para los dos subsets\n",
    "high_energy_mae= np.mean(np.abs(high_energy_prediction-y_full[high_energy_rows]))\n",
    "high_energy_mse= np.mean((high_energy_prediction-y_full[high_energy_rows])**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora imprimimos los resultados y comprobamos si el modelo predice mejor cuando los valores de energía son altos o bajos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MÉTRICAS PARA VALORES DE ALTA ENERGÍA\n",
      " \n",
      "Error absoluto medio para valores de alta energía\n",
      " 4.763981962342188e-05\n",
      "Error cuadrático medio para valores de alta energía\n",
      " 1.6382511675874368e-08\n",
      "\n",
      "MÉTRICAS PARA VALORES DE BAJA ENERGÍA\n",
      " \n",
      "Error absoluto medio para valores de baja energía\n",
      " 1.138692306768182e-05\n",
      "Error cuadrático medio para valores de baja energía\n",
      " 5.225428846854651e-09\n",
      "\n",
      "El error absoluto medio es mayor en valores altos de energía\n",
      "\n",
      "El error cuadrático medio es mayor en valores altos de energía\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar resultados\n",
    "print(\"MÉTRICAS PARA VALORES DE ALTA ENERGÍA\\n \")\n",
    "print(\"Error absoluto medio para valores de alta energía\\n\",high_energy_mae)\n",
    "print(\"Error cuadrático medio para valores de alta energía\\n\",high_energy_mse)\n",
    "\n",
    "\n",
    "low_energy_mae=np.mean(np.abs(low_energy_prediction)-y_full[low_energy_rows])\n",
    "low_energy_mse= np.mean((low_energy_prediction-y_full[low_energy_rows])**2)\n",
    "\n",
    "\n",
    "print(\"\\nMÉTRICAS PARA VALORES DE BAJA ENERGÍA\\n \")\n",
    "print(\"Error absoluto medio para valores de baja energía\\n\",low_energy_mae)\n",
    "print(\"Error cuadrático medio para valores de baja energía\\n\",low_energy_mse)\n",
    "\n",
    "if high_energy_mae>low_energy_mae:\n",
    "    print(\"\\nEl error absoluto medio es mayor en valores altos de energía\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nEl error absoluto medio es mayor en valores bajos de energía\\n\")\n",
    "\n",
    "if high_energy_mse>low_energy_mse:\n",
    "    print(\"El error cuadrático medio es mayor en valores altos de energía\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"El error cuadrático medio es mayor en valores bajos de energía\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte convertiremos el problema de regresión en uno de clasificación. Para ello, definiremos un threshold de energía(el tercer cuantil), a partir del cual los valores de energía serán considerados altos, y los que sean menores que este threshold serán considerados bajos.\n",
    "\n",
    "Usaremos algunos de los valores de la parte 1, ya que en ella también hemos usado el threshold del tercer cuantil y hemos creado los subsets de energía alta y baja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de la accuracy para los diferentes scalers con KNN(outer evaluation):\n",
      "StandardScaler: 0.8476\n",
      "MinMaxScaler: 0.8462\n",
      "RobustScaler: 0.8507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Crear una columna de categoría. Los valores con energía menor que el tercer cuantil\n",
    "#son de la categoría low, y los demás son de la categoría high.\n",
    "categories=pd.cut(wind_ava_sotavento[\"energy\"],bins=[float('-inf'),energy_threshold,float('inf')],labels=['low','high'])\n",
    "wind_ava_sotavento=wind_ava_sotavento.copy()\n",
    "wind_ava_sotavento.loc[:,\"category\"]= categories\n",
    "wind_ava_sotavento=wind_ava_sotavento.drop('energy',axis=1)\n",
    "# Hacemos split del dataset en features y target variable\n",
    "X = wind_ava_sotavento.drop('category', axis=1)\n",
    "y = wind_ava_sotavento['category']\n",
    "\n",
    "# Hacemos split del dataset en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=438770)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=438770)\n",
    "# Definir scalers\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"RobustScaler\": RobustScaler()\n",
    "}\n",
    "\n",
    "# Diccionario para guardar la puntuación de cross validation de cada scaler\n",
    "outer_scores = {}\n",
    "\n",
    "# Loop through the scalers\n",
    "for name, scaler in scalers.items():\n",
    "    # Create a pipeline with scaler and KNN\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    accuracy_scores = cross_val_score(pipeline, X_train, y_train, scoring='accuracy', cv=outer_cv)\n",
    "    #Obtenemos la media de accuracy de cada scaler\n",
    "    \n",
    "    outer_scores[name] = accuracy_scores.mean()  \n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(\"\\nResumen de la accuracy para los diferentes scalers con KNN(outer evaluation):\")\n",
    "for scaler, accuracy in outer_scores.items():\n",
    "    print(f\"{scaler}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, el robust scaler es el más preciso para esta tarea. Por lo tanto, este será el que usaremos en este problema de clasificación a partir de ahora. \n",
    "\n",
    "Lo siguiente que haremos es elegir la mejor técnica de clasificación para intentar encontrar el modelo más apropiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Usaremos TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Lista de pipelines para evaluar\n",
    "models = {\n",
    "    'KNN': make_pipeline(RobustScaler(), KNeighborsClassifier()),\n",
    "    'DecisionTreeClassifier': make_pipeline(RobustScaler(), DecisionTreeClassifier(random_state=438770)),\n",
    "    'LogisticRegressionNormal': make_pipeline(RobustScaler(), LogisticRegression(max_iter=1000)),\n",
    "    'SVM': make_pipeline(RobustScaler(), SVC())\n",
    "}\n",
    "\n",
    "\n",
    "# Diccionario para guardar los resultados\n",
    "cv_results = {}\n",
    "\n",
    "# Evaluamos los modelos\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    accuracy_scores = cross_validate(model, X, y, cv=tscv, scoring='accuracy', return_train_score=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_accuracy = accuracy_scores['test_score'].mean()  \n",
    "    avg_time = (accuracy_scores['fit_time']).mean()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    cv_results[name] = {\n",
    "        'Accuracy': avg_accuracy,\n",
    "        'Average Fit Time': avg_time,\n",
    "        'Tiempo Evaluación Total': total_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "    Tiempo de evaluación total: 0.8456 segundos\n",
      "    Average Fit Time: 0.0101 seconds\n",
      "    Accuracy media: 0.8450\n",
      "\n",
      "DecisionTreeClassifier:\n",
      "    Tiempo de evaluación total: 0.4277 segundos\n",
      "    Average Fit Time: 0.0764 seconds\n",
      "    Accuracy media: 0.8099\n",
      "\n",
      "LogisticRegressionNormal:\n",
      "    Tiempo de evaluación total: 0.3784 segundos\n",
      "    Average Fit Time: 0.0582 seconds\n",
      "    Accuracy media: 0.8063\n",
      "\n",
      "SVM:\n",
      "    Tiempo de evaluación total: 1.4264 segundos\n",
      "    Average Fit Time: 0.1171 seconds\n",
      "    Accuracy media: 0.8491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def print_results(results):\n",
    "    for model, metrics in results.items():\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"    Tiempo de evaluación total: {metrics['Tiempo Evaluación Total']:.4f} segundos\")\n",
    "        print(f\"    Average Fit Time: {metrics['Average Fit Time']:.4f} seconds\")\n",
    "        print(f\"    Accuracy media: {metrics['Accuracy']:.4f}\\n\")\n",
    "\n",
    "print_results(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANÁLISIS\n",
    "Se puede ver que SVM tiene la mejor accuracy hasta ahora, aunque el tiempo de entrenamiento es mayor que en otras alternativas como KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AJUSTE DE HIPERPARÁMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "\n",
    "# Diccionario para guardar los resultados de cada modelo\n",
    "results = {\n",
    "\n",
    "}\n",
    "\n",
    "# Inner cross-validation será KFold con 5 splits\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=438770)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para KNN: {'knn__algorithm': 'auto', 'knn__n_neighbors': 22, 'knn__weights': 'distance'}\n",
      "Mejor accuracy para KNN después de HPO: 0.8623\n",
      "Training time de HPO para KNN: 110.4324 segundos\n"
     ]
    }
   ],
   "source": [
    "# Pipeline para KNN\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Grid de hiperparámetros para KNN\n",
    "knn_params = {\n",
    "    'knn__n_neighbors': range(1, 31), \n",
    "    'knn__weights': ['uniform', 'distance'],  \n",
    "    'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  \n",
    "}\n",
    "\n",
    "# Algoritmo de GridSearch para encontrar los mejores hiperparámetros\n",
    "knn_grid_search = GridSearchCV(knn_pipeline, knn_params, cv=inner_cv, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Iniciamos el timer\n",
    "start_time = time.time()\n",
    "knn_grid_search.fit(X, y)  \n",
    "end_time = time.time()\n",
    "\n",
    "# Mejores parámetros y accuracy\n",
    "best_params = knn_grid_search.best_params_\n",
    "best_accuracy = knn_grid_search.best_score_\n",
    "training_time_hpo = end_time - start_time\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"Mejores parámetros para KNN: {best_params}\")\n",
    "print(f\"Mejor accuracy para KNN después de HPO: {best_accuracy:.4f}\")\n",
    "print(f\"Training time de HPO para KNN: {training_time_hpo:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Mejores parámetros para DecisionTreeClassifier: {'tree__max_depth': 4, 'tree__min_samples_split': 2}\n",
      "Mejor accuracy para DecisionTreeClassifier después de HPO: 0.8629\n",
      "Tiempo de entrenamiento de HPO para DecisionTreeClassifier: 90.8962 segundos\n"
     ]
    }
   ],
   "source": [
    "# Pipeline para el TreeClassifier\n",
    "tree_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()), \n",
    "    ('tree', DecisionTreeClassifier(random_state=438770))\n",
    "])\n",
    "\n",
    "# Definición de hiperparámetros\n",
    "tree_params = {\n",
    "    'tree__max_depth': range(1, 21), \n",
    "    'tree__min_samples_split': range(2, 11), \n",
    "}\n",
    "\n",
    "# GridSearchCV para DecisionTree\n",
    "tree_grid_search = GridSearchCV(tree_pipeline, tree_params, cv=inner_cv, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Timing the hyperparameter optimization process\n",
    "start_time = time.time()\n",
    "tree_grid_search.fit(X, y)  \n",
    "end_time = time.time()\n",
    "\n",
    "# Extracting the results\n",
    "best_params_tree = tree_grid_search.best_params_\n",
    "best_accuracy_tree = tree_grid_search.best_score_\n",
    "training_time_tree_hpo = end_time - start_time\n",
    "\n",
    "# Output the results for the Regression Tree\n",
    "print(f\"Mejores parámetros para DecisionTreeClassifier: {best_params_tree}\")\n",
    "print(f\"Mejor accuracy para DecisionTreeClassifier después de HPO: {best_accuracy_tree:.4f}\")\n",
    "print(f\"Tiempo de entrenamiento de HPO para DecisionTreeClassifier: {training_time_tree_hpo:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para LogisticRegression: {'logistic_regression__C': 10, 'logistic_regression__penalty': 'l1', 'logistic_regression__solver': 'liblinear'}\n",
      "Mejor accuracy para LogisticRegression después de HPO: 0.8193\n",
      "HPO training time para LogisticRegression: 129.9647 segundos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "# Adjust the Lasso Regression pipeline\n",
    "logistic_regression_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('logistic_regression', LogisticRegression(max_iter=10000))  # Initially setting max_iter to 10,000\n",
    "])\n",
    "\n",
    "# Grid de hiperparámetros para LogisticRegression\n",
    "logistic_regression_params = {\n",
    "    'logistic_regression__penalty': ['l1', 'l2'],                # Regularization penalty\n",
    "    'logistic_regression__C': [0.001, 0.01, 0.1, 1, 10, 100],    \n",
    "    'logistic_regression__solver': ['liblinear', 'saga']        # Algoritmo de optimización\n",
    "}\n",
    "\n",
    "# GridSearchCV para logistic regression\n",
    "logistic_regression_grid_search = GridSearchCV(logistic_regression_pipeline, logistic_regression_params, cv=inner_cv, scoring='accuracy', verbose=1,n_jobs=-1)\n",
    "\n",
    "# Comenzar optimización de hiperparámetros\n",
    "start_time = time.time()\n",
    "logistic_regression_grid_search.fit(X, y)  \n",
    "end_time = time.time()\n",
    "\n",
    "# Extraemos resultados\n",
    "best_params_logistic_regression = logistic_regression_grid_search.best_params_\n",
    "best_accuracy_logistic_regression=logistic_regression_grid_search.best_score_\n",
    "training_time_logistic_regression_hpo = end_time - start_time\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"Mejores parámetros para LogisticRegression: {best_params_logistic_regression}\")\n",
    "print(f\"Mejor accuracy para LogisticRegression después de HPO: {best_accuracy_logistic_regression:.4f}\")\n",
    "print(f\"HPO training time para LogisticRegression: {training_time_logistic_regression_hpo:.4f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para SVM: {'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "Mejor accuracy para SVM después de HPO: 0.8774\n",
      "Tiempo de HPO para SVM: 112.1333 segundos\n"
     ]
    }
   ],
   "source": [
    "# Definimos la técnica de inner cross validation\n",
    "\n",
    "\n",
    "# Pipeline para SVM\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),  \n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid for SVM\n",
    "svm_params = {\n",
    "    'svm__C': [0.1,1,10,100], \n",
    "    'svm__kernel': ['linear', 'poly', 'rbf'], \n",
    "    'svm__gamma': ['scale', 'auto'],  \n",
    "}\n",
    "\n",
    "# GridSearchCV para SVM\n",
    "svm_grid_search = GridSearchCV(svm_pipeline, svm_params, cv=inner_cv, scoring='accuracy', verbose=1,n_jobs=-1)\n",
    "\n",
    "# Iniciamos el timer para optimización de hiperparámetros\n",
    "start_time = time.time()\n",
    "svm_grid_search.fit(X, y) \n",
    "end_time = time.time()\n",
    "\n",
    "# Extraemos los mejores parámetros y el RMSE\n",
    "best_params_svm = svm_grid_search.best_params_\n",
    "best_accuracy_svm = svm_grid_search.best_score_\n",
    "training_time_svm_hpo = end_time - start_time\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(f\"Mejores parámetros para SVM: {best_params_svm}\")\n",
    "print(f\"Mejor accuracy para SVM después de HPO: {best_accuracy_svm:.4f}\")\n",
    "print(f\"Tiempo de HPO para SVM: {training_time_svm_hpo:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado modelos:\n",
      "\n",
      "KNN Resultados:\n",
      "    Mejores Parámetros: {'knn__algorithm': 'auto', 'knn__n_neighbors': 22, 'knn__weights': 'distance'}\n",
      "    Mejor Accuracy: 0.8623\n",
      "    Tiempo entrenamiento HPO: 110.4324 seconds\n",
      "\n",
      "ClassificationTree Resultados:\n",
      "    Mejores Parámetros: {'tree__max_depth': 4, 'tree__min_samples_split': 2}\n",
      "    Mejor Accuracy: 0.8629\n",
      "    Tiempo entrenamiento HPO: 90.8962 seconds\n",
      "\n",
      "SVM Resultados:\n",
      "    Mejores Parámetros: {'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "    Mejor Accuracy: 0.8774\n",
      "    Tiempo entrenamiento HPO: 112.1333 seconds\n",
      "\n",
      "Logistic regression Resultados:\n",
      "    Mejores Parámetros: {'logistic_regression__C': 10, 'logistic_regression__penalty': 'l1', 'logistic_regression__solver': 'liblinear'}\n",
      "    Mejor Accuracy: 0.8193\n",
      "    Tiempo entrenamiento HPO: 129.9647 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing results for each model in the 'results' dictionary\n",
    "results['KNN'] = (best_params, best_accuracy, training_time_hpo)\n",
    "results['ClassificationTree'] = (best_params_tree, best_accuracy_tree, training_time_tree_hpo)\n",
    "results['SVM'] = (best_params_svm, best_accuracy_svm, training_time_svm_hpo)\n",
    "results['Logistic regression'] = (best_params_logistic_regression, best_accuracy_logistic_regression, training_time_logistic_regression_hpo)\n",
    "\n",
    "print(\"Resultado modelos:\\n\")\n",
    "for model_name, (params, accuracy, time) in results.items():\n",
    "    print(f\"{model_name} Resultados:\")\n",
    "    print(f\"    Mejores Parámetros: {params}\")\n",
    "    print(f\"    Mejor Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    Tiempo entrenamiento HPO: {time:.4f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basándonos en los resultados presentados, el modelo SVM muestra la mayor precisión media después de la optimización de hiperparámetros (0.8657), con un tiempo de entrenamiento de HPO bastante elevado. Sin embargo, al utilizar el parámetro n_jobs=-1(que permite al programa usar más cores de la CPU) este tiempo se ve disminuido. Por lo tanto, el modelo SVM podría considerarse el mejor en términos de precisión para el problema de clasificación en cuestión. \n",
    "\n",
    "La LogisticRegression, a pesar de tener un tiempo de entrenamiento de HPO más largo, no proporciona una precisión significativamente mejor que los otros modelos, lo que la convierte en una opción menos atractiva en términos de tiempo de entrenamiento y rendimiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente que haremos es comparar los resultados de nuestro mejor modelo (SVM) con los obtenidos por un DummyClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de elementos de cada clase en nuestro dataset:\n",
      "category\n",
      "low     3561\n",
      "high    1187\n",
      "Name: count, dtype: int64\n",
      "Precisión para DummyClassifier: 0.7674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=438770)\n",
    "class_counts= wind_ava_sotavento['category'].value_counts()\n",
    "print(\"Número de elementos de cada clase en nuestro dataset:\")\n",
    "print(class_counts)\n",
    "# Creamos el dummy regressor\n",
    "dummy_regressor = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Entrenamos el dummy regressor\n",
    "dummy_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Hacemos predicciones en el test set\n",
    "y_pred = dummy_regressor.predict(X_test)\n",
    "\n",
    "# Calculamos accuracy para el dummy regressor\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(f\"Precisión para DummyClassifier: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, nuestro mejor modelo supera la precisión del DummyClassifier y, de hecho, todos los modelos que hemos probado la superan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTER EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección veremos cómo se comporta el modelo escogido, SVM, al realizar predicciones sobre el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Evaluation - Accuracy en el Test Set para SVM: 0.8958\n"
     ]
    }
   ],
   "source": [
    "# Creación de Pipeline de SVM\n",
    "final_svm_model = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('svm', SVC(C=100, gamma='scale', kernel='rbf'))\n",
    "])\n",
    "\n",
    "# Entrenamos con el train set\n",
    "final_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacemos predicciones en el test set\n",
    "y_pred_svm = final_svm_model.predict(X_test)\n",
    "\n",
    "# Calculamos la precisión de nuestro modelo \n",
    "accuracy_svm = accuracy_score(y_test,y_pred_svm)\n",
    "print(f\"Outer Evaluation - Accuracy en el Test Set para SVM: {accuracy_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que la precisión es alta y podemos concluir que el modelo es efectivo a la hora de clasificar elementos.\n",
    "\n",
    "El modelo escogido estaría definido, por lo tanto, como: SVC(C=100,gamma='scale',kernel='rbf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
